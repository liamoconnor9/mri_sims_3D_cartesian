\documentclass[longbibliography,twocolumn,amsmath,amssymb,aps,nofootinbib]{revtex4-2}

\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{gensymb}
\usepackage{enumerate}
\usepackage{varwidth}
\usepackage{float}
\usepackage{nonfloat}
\usepackage{soul}
\usepackage{ulem}
% \usepackage{multicol}
% \usepackage{afterpage}
\usepackage[usenames, dvipsnames]{color}
\newcommand{\note}[1]{\noindent \textbf{\textit{\textcolor{Red}{#1}}}}

\newcommand\Ra{\mathrm{Ra}}
\newcommand\Pran{\mathrm{Pr}}
\newcommand\Rac{\mathrm{Ra}_{\mathrm{c}}}
\newcommand\Ek{\mathrm{Ek}}
\newcommand\Ro{\mathrm{Ro}}
\newcommand\Nu{\mathrm{Nu}}
\newcommand\Sc{\mathrm{Sc}}

\newcommand\eps{\varepsilon}
\renewcommand\L {\mathcal{L}}
\renewcommand{\citet}[1]{ref.~\cite{#1}}
\renewcommand{\Citet}[1]{Ref.~\cite{#1}}
\newcommand{\citets}[1]{refs.~\cite{#1}}
\newcommand{\Citets}[1]{Refs.~\cite{#1}}
\newcommand{\davg}[1]{\langle {#1} \rangle}
\newcommand{\n}{\\ \nonumber \\ }
\newcommand{\nn}{\nonumber}
\newcommand{\nnn}{\\ \nonumber \\ \nonumber}

\newcommand\ie{\textit{i.e.},~}
\newcommand\eg{\textit{e.g.},~}
\newcommand{\omicron}{o}

\newcommand{\pd}[1]{\partial_{#1}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\M}[1]{\mathbf{#1}}
\newcommand{\grad}{\vec{\nabla}}
\newcommand{\cross}{\vec{\times}}
\newcommand{\laplacian}{\nabla^2}

\newcommand{\sump}[2]{\sideset{}{'}\sum_{{#1}=0}^{#2}}

\newcommand{\eq}[1]{(\ref{#1})}
\newcommand{\eqs}[2]{(\ref{#1})~\&~(\ref{#2})}
\newcommand{\eqss}[2]{(\ref{#1})--(\ref{#2})}

\newcommand{\Eq}[1]{Eq.~(\ref{#1})}
\newcommand{\Eqs}[2]{Eqs.~(\ref{#1})~\&~(\ref{#2})}
\newcommand{\Eqss}[2]{Eqs.~(\ref{#1})--(\ref{#2})}

\newcommand{\fig}[1]{Fig.~(\ref{#1})}
\newcommand{\figs}[2]{Figs.~(\ref{#1})~\&~(\ref{#2})}
\newcommand{\T}{{\cal T}}
\newcommand{\Z}{{\cal Z}}


\makeatletter
\let\Hy@backout\@gobble
\makeatother

\newcommand*{\GtrSim}{\smallrel\gtrsim}

\makeatletter
\newcommand*{\smallrel}[2][.8]{%
  \mathrel{\mathpalette{\smallrel@{#1}}{#2}}%
}
\newcommand*{\smallrel@}[3]{%
  % #1: scale factor
  % #2: math style
  % #3: symbol
  \sbox0{$#2\vcenter{}$}%
  \dimen@=\ht0 %
  \raise\dimen@\hbox{%
    \scalebox{#1}{%
      \raise-\dimen@\hbox{$#2#3\m@th$}%
    }%
  }%
}
\makeatother


\begin{document}

\title{Least-Squares Time Reversal of PDEs}

\author{Liam O'Connor$^{1,2}$}
\affiliation{%
$^1$Department of Engineering Sciences and Applied Mathematics, Northwestern University, Evanston, IL 60208 USA}
\affiliation{%
$^2$Center for Interdisciplinary Exploration and Research in Astrophysics, Northwestern University, Evanston, IL, 60201 USA}

\begin{abstract}
    abstract
\end{abstract}

\maketitle

\section{Introduction}
Rayleigh-B\'enard convection plays a foundational role in astrophysical and geophysical settings.
The resulting buoyancy-driven flows regulate heat transfer and generate large-scale vortices \cite{Couston}.
Turbulent convection, which is associated with large Rayleigh numbers $\Ra$, is difficult to simulate. 
State of the art simulations performed by \cite{Zhu_2018} have reached $\Ra \sim 10^{14}$ but estimates for the sun's convective zone and earth's interior are $\Ra \sim 10^{16}-10^{20}$ and $\Ra \sim 10^{20}-10^{30}$ respectively \cite{Ossendrijver,Gubbins_2001}. 

\clearpage
\section{Generalized Problem Setup}
Given a generalized initial value problem
\begin{align}
  \mathcal{F}[U] &\equiv \partial_t U + L[U] + N[U, U] = 0 \label{genpde}
  \intertext{and a target end state $U(x, T)$, our aim is to recover the associated initial condition $U(x, 0)$.  
  This is realized by performing gradient-based minimization of the functional}
  H[u, U] &= \frac{1}{2}\davg{(u(x, T) - U(x, T))^2} = \frac{1}{2}\davg{u'(x, T)^2} \label{Hdef}, 
\end{align}
where the operator $\davg{\cdot}$ denotes the domain-integrated average and the quantity $u'(x, t)$ refers to $u(x, t)$'s the deviation from the target solution. For the remainder of this paper, capital symbols (such as $U(x, t)$) will be used to denote the target solution, while their corresponding lowercase symbols (such as $u(x, t)$) play the role of optimization parameters.

The Lagrangian associated with our initial value problem is given by
\begin{align*}
  \mathcal{L} &= \int_0^T \davg{\mu \cdot \mathcal{F}[u]} dt + H[u, U].
  \intertext{Where $\mu(x, t)$ is the Lagrangian Multiplier corresponding to $u(x, t)$. Equating} \frac{\delta\L}{\delta u} &= 0 \quad \text{and} \quad \frac{\delta\L}{\delta \mu} = 0 
  \intertext{yields the original and adjoint systems} 
  \mathcal{F}[u] &= 0 \quad \text{and} \quad \mathcal{F}^{\dagger}[\mu, u] = 0 \intertext{respectively. At some particular guess $u^n(x, 0)$, we compute the functional derivative}
  \frac{\delta\L}{\delta u^n(x, 0)} &= -\mu^n(x, 0)
  \intertext{to determine the direction of steepest descent of our objective functional $H[u, U]$ with respect to the optimization parameter $u(x, 0)$. Naive gradient-descent can then be performed by computing an updated guess}
  u^{n+1}(x, 0) &= u^n(x, 0) + \varepsilon \mu^n(x, 0)
  \intertext{where $\varepsilon$ is the step size.}
\end{align*}
In addition to this algorithm, more advanced gradient-based optimization techniques such as conjugate gradient and BFGS have been employed in conjunction with adjoint-looping to improve performance and accelerate convergence.
In the context of our time-inversion optimization problem, such techniques traverse the space of possible initial conditions $u(x, 0)$ by relying on local information pertaining to the objective functional $H$ and its functional derivative $\frac{\delta H}{\delta u(x, 0)}$. 
For this reason, the focus of our investigation is largely centered around the objective functional's dependence on the initial condition.
More specifically, we study the levelsets of the objective functional because these surfaces characterize the directions of steepest descent at any arbitrary point in the space of initial conditions.
In doing so, we highlight the challenges and deficits associated with using the least squares functional to recover the initial condition $U(x, 0)$ belonging to a target end state $U(x, T)$.


% Here I'll present a generalized PDE which we want to invert along with a bunch of notation. We'll define our target simulation, least-squares objective, and cite examples where the adjoint looping method has been successfully applied to inverse problems. We'll conclude this discussion by stressing the important of the objective function's shape in ``initial condition space'', because ultimately this is what curtails our gradient-based inversion technique.

\section{Analytic Analysis}
\subsection{Linearity and Reversiblity}
In this section, we study linear initial value problems which obey (\ref{genpde}) along with the property $\mathcal{F}[u'] = \mathcal{F}[u] - \mathcal{F}[U]$. 
In these special cases, the objective functional (\ref{Hdef}) depends only on the deviation's initial condition $u'(x, 0)$ about any arbitrary target $U$.
In the following section, we demonstrate that the objective functionals of nonlinear systems with nonzero targets are directory influenced by one's choice of $U$.

time-reversible initial value problems are trivial cases of this optimization problem. 
Linear systems' objective functions' shapes do not depend on the target--the target manifests itself as a constant shift in ic space. 
Thus we might as well consider problems with $U(0) = U(T) = 0$ for all $x$.
Reversible wave equations, which conserve energy yield objective functions whose levelsets in ic space are spherical, so the gradient at any given guess always provides the shortest path to the target. 
In contrast, irreversible diffusive terms distort the spherical levelsets into hyperellipses, whose semi-minimum (maximum) axis is oriented in the direction of the lowest (highest) wavenumber fourier mode's coefficient.
Thus, when traversing an arbitrary path of steepest descent toward the target IC, low wavenumber modes are matched with the target first, followed by increasingly higher wavenumber modes.

\subsection{Nonlinear Reversible Problems}
Here the target matters. We consider burger's equation
\begin{align*}
  \partial_t u + u\partial_x u &= 0
  \intertext{where energy is conserved}
  0 &= \partial_t \langle \frac{1}{2}u^2 \rangle + \langle u^2\partial_x u\rangle \\
  &= \partial_t \langle \frac{1}{2}u^2 \rangle + \langle  \frac{1}{3}\partial_x u^3\rangle \\
  &= \partial_t \langle \frac{1}{2}u^2 \rangle \\
\end{align*}
Next we consider a guess with a known deviation from the target $u(x, 0) = U(x, 0) + u'(x, 0)$. Our objective function is given by $\langle (u(T) - U(T))^2 \rangle = \langle u'(T)^2 \rangle$. The forward evolution equation for the perturbation is given by

\begin{align*}
  0 &= \partial_t u' + u'\partial_x U + U\partial_x u' + u'\partial_x u' \\
  \intertext{then we have that}
  \intertext{the evolution of the target is given by}
  0 &= \frac{1}{2}\partial_t \davg{u'^2} + \davg{u' \partial_x (u'U)} + \davg{\frac{1}{3}\partial_x u'^3} \\
  &= \frac{1}{2}\partial_t \davg{u'^2} + \davg{u' \partial_x (u'U)} \\
  &= \frac{1}{2}\partial_t \davg{u'^2} + \davg{U \partial_x (u'^2)} \\
  \intertext{Suppose $u' = \alpha U$. In this case $u = U + u' = (1+\alpha)U$ evolves identically to $U$ but at a rate which is $(1 + \alpha)^2$ faster. Thus the objective is given by $\davg{(U((1+\alpha)^2 T) - U(T))^2}$}.
  % \intertext{Suppose the perturbation is small enough that its nonlinear term can be neglected. In this case we find that}
  u'(T) &= u'(0) + \int_0^T \partial_tu'dt \\
  &= u'(0)-\int_0^T \partial_x (u'U) + \frac{1}{2}\partial_x (u'^2) dt
  \intertext{Therefore $\davg{u'(T)} = \davg{u'(0)}$. The objective function}
  \davg{u'(T)^2}&= \davg{u'(0)^2}- \davg{u'(0)\int_0^T \partial_x (u'U) + \frac{1}{2}\partial_x (u'^2) dt} \\
  &\;\; + \davg{\Big[\int_0^T \partial_x (u'U) + \frac{1}{2}\partial_x (u'^2) dt\Big]^2} \\
\end{align*}


\clearpage
\bibliography{lsrtm.bib}

\end{document}
